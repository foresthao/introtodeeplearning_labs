{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LRP.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "WnfeDR02gDAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "5ea5a0c5-580c-4369-b3c9-cdd6a7751115"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/rabbit159t/introtodeeplearning_labs.git\n",
        "% cd introtodeeplearning_labs/test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'introtodeeplearning_labs'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 68 (delta 16), reused 62 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n",
            "/content/introtodeeplearning_labs/test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pkPP6S9rgDAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "d0db0116-cb39-49a8-9cae-cbded99c82c8"
      },
      "cell_type": "code",
      "source": [
        "# %load LRP.py\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt  \n",
        "import numpy as np\n",
        "import pickle\n",
        "import utils\n",
        "\n",
        "\n",
        "\n",
        "def rot180(ndarray):\n",
        "    rot= np.flip(np.flip(ndarray, 1), 0)\n",
        "    return rot\n",
        "\n",
        "class LayerwiseRelevancePropagation:\n",
        "    def __init__(self, model_name='32CNN.h5', alpha=2, epsilon=1e-9):\n",
        "        self.model = load_model(model_name)\n",
        "        self.alpha = alpha\n",
        "        self.beta = 1 - alpha\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.names, self.activations, self.weights = utils.get_model_params(self.model)\n",
        "        self.num_layers = len(self.names)\n",
        "\n",
        "        self.relevance= self.compute_relevances()\n",
        "        self.lrp_runner = K.function(inputs=[self.model.input, ], outputs=[self.relevance, ])\n",
        "\n",
        "\n",
        "    def print_model(self):\n",
        "        print('---------------------------------')\n",
        "        print(self.names[0])\n",
        "\n",
        "        print(self.model.input) #Tensor(\"conv1d_93_input:0\", shape=(?, 120, 1), dtype=float32)\n",
        "        #print(self.model.output) #(\"dense_31/Softmax:0\", shape=(?, 16), dtype=float32)\n",
        "        print(self.activations[0]) #Tensor(\"conv1d_93_input:0\", shape=(?, 120, 1), dtype=float32)\n",
        "        print(self.relevance) #Tensor(\"mul_11:0\", shape=(?, 120, 1), dtype=float32)\n",
        "        print(self.weights[0])\n",
        "        print('---------------------------------')\n",
        "        print('==============names==============')      \n",
        "        for i in self.names:\n",
        "            print('##', len(self.names))\n",
        "            print (i)\n",
        "        print('==============activation==============')  \n",
        "        for x in self.activations:\n",
        "            print('##', len(self.activations))\n",
        "            print(type(x))\n",
        "        print('==============weight==============') \n",
        "        for x in self.weights:\n",
        "            print('##', len(self.weights))\n",
        "            print(type(x))\n",
        "\n",
        "    def compute_relevances(self):\n",
        "        r= self.model.output\n",
        "        for i in range(self.num_layers-2, -1, -1):\n",
        "            #print(i+1)\n",
        "            #print('vist:',self.names[i + 1])\n",
        "        \n",
        "            if 'fc' in self.names[i + 1]:\n",
        "                #print('===================== compute_relevances fc=====================')\n",
        "                r = self.backprop_fc(self.weights[i + 1][0], self.weights[i + 1][1], self.activations[i], r)\n",
        "                #print('########fc Relevances: ######## ', r)\n",
        "            elif 'flatten' in self.names[i + 1]:\n",
        "                #print('=====================compute_relevances flatten=====================')\n",
        "                r = self.backprop_flatten(self.activations[i], r)\n",
        "                #print('########flatten Relevances: ######## ', r)\n",
        "            elif 'conv' in self.names[i + 1]:\n",
        "                #print('=====================compute_relevances conv=====================')\n",
        "                r = self.backprop_conv(self.weights[i + 1][0], self.weights[i + 1][1], self.activations[i], r)\n",
        "                #print('########conv Relevances: ######## ', r)\n",
        "\n",
        "        return r\n",
        "\n",
        "\n",
        "    def first_fc(self, weights, bias, activations, relevances):\n",
        "        lowest= -1.0\n",
        "        highest= 1.0    \n",
        "        V= K.maximum(weights, 0.)\n",
        "        U= K.minimum(weights, 0.)\n",
        "        L= activations*0+lowest\n",
        "        H= activations*0+highest\n",
        "\n",
        "        Z= K.dot(activations, weights)- K.dot(L, V)-K.dot(H, U)+self.epsilon\n",
        "        S= relevance/Z\n",
        "        return(activations*K.dot(S, transpose(weights))- L*K.dot(S, transpose(V)) - H*K.dot(S, transpose(U)))\n",
        "\n",
        "    def backprop_fc(self, weights, bias, activations, relevances):\n",
        "        '''\n",
        "        V= K.maximum(weights, 0.)\n",
        "        Z = K.dot(activations,V)+ self.epsilon\n",
        "        S = relevances/Z\n",
        "        C =  K.dot(S, K.transpose(V))\n",
        "        print('===================== backprop_fc finished ==========================')\n",
        "        return (activations*C)\n",
        "        '''\n",
        "        w_p = K.maximum(weights, 0.)\n",
        "        b_p = K.maximum(bias, 0.)\n",
        "\n",
        "        z_p = K.dot(activations, w_p) + b_p + self.epsilon\n",
        "        \n",
        "        s_p = relevances / z_p\n",
        "\n",
        "        c_p = K.dot(s_p, K.transpose(w_p))\n",
        "\n",
        "        w_n = K.minimum(weights, 0.)\n",
        "        b_n = K.maximum(bias, 0.)\n",
        "        z_n = K.dot(activations, w_n) + b_n - self.epsilon\n",
        "        s_n = relevances / z_n\n",
        "        c_n = K.dot(s_n, K.transpose(w_n))\n",
        "        print('===================== backprop_fc finished ==========================')\n",
        "        return activations * (self.alpha * c_p + self.beta * c_n)\n",
        "\n",
        "\n",
        "    def backprop_flatten(self, a, r):\n",
        "        shape = a.get_shape().as_list()\n",
        "        shape[0] = -1\n",
        "        #print('===================== backprop_flatten finished ==========================')\n",
        "        return K.reshape(r, shape)\n",
        "\n",
        "    def first_conv(self, w, b, a, r):\n",
        "\n",
        "        w_p = K.maximum(w, 0.)\n",
        "        b_p = K.maximum(b, 0.)\n",
        "\n",
        "        z_p = K.conv1d(a, kernel=w_p, strides=1, padding='valid') + b_p + self.epsilon  \n",
        "\n",
        "        s_p = r / z_p\n",
        "\n",
        "        c_p = tf.contrib.nn.conv1d_transpose(\n",
        "                value= s_p,\n",
        "                filter= w_p, \n",
        "                output_shape= K.shape(a),\n",
        "                stride= 1,\n",
        "                padding='SAME',\n",
        "                name=None\n",
        "                )\n",
        "\n",
        "        w_n = K.minimum(w, 0.)\n",
        "        b_n = K.minimum(b, 0.)\n",
        "        z_n = K.conv1d(a, kernel=w_n, strides=1, padding='valid') + b_n - self.epsilon\n",
        "        s_n = r / z_n\n",
        "        c_n = tf.contrib.nn.conv1d_transpose(\n",
        "                value= s_n,\n",
        "                filter= w_n,\n",
        "                output_shape= K.shape(a),\n",
        "                stride= 1,\n",
        "                padding='SAME',\n",
        "                name=None\n",
        "                )\n",
        "\n",
        "        return a * (self.alpha * c_p + self.beta * c_n)\n",
        "\n",
        "\n",
        "    '''\n",
        "        lowest= -1.0\n",
        "        highest= 1.0\n",
        "        w_p = K.maximum(w, 0.)\n",
        "        w_n = K.minimum(w, 0.)\n",
        "        iself = keras.models.clone_model(self.model)\n",
        "        nself = keras.models.clone_model(self.model)\n",
        "        pself = keras.models.clone_model(self.model)\n",
        "\n",
        "        K.set_value(iself.layers[0].weights[1], np.zeros((16,)))\n",
        "\n",
        "        K.set_value(nself.layers[0].weights[1], np.zeros((16,)))\n",
        "        #K.set_value(nself.layers[0].weights[0], w_n)\n",
        "\n",
        "        K.set_value(pself.layers[0].weights[1], np.zeros((16,)))\n",
        "        #K.set_value(pself.layers[0].weights[0], w_p)\n",
        "\n",
        "        X, L, H= a, X*0*lowest, X*0+highest\n",
        "\n",
        "        zn= K.conv1d(X, kernel=w_n, strides=1, padding='valid')\n",
        "        zp= K.conv1d(X, kernel=w_p, strides=1, padding='valid')\n",
        "        z=  K.conv1d(X, kernel=w, strides=1, padding='valid')   \n",
        "\n",
        "\n",
        "\n",
        "        Z=  K.conv1d(X, kernel=w, strides=1, padding='valid')  \n",
        "        Z = K.conv1d(a, kernel=w_p, strides=1, padding='valid') + b_p + self.epsilon\n",
        "        self.lrp_runner = K.function(inputs=[self.model.input, ], outputs=[self.relevance, ])\n",
        "        \n",
        "        w_p = K.maximum(w, 0.)\n",
        "        print('************', w_p)\n",
        "        b_p = K.maximum(b, 0.)\n",
        "\n",
        "        z = K.conv1d(a, kernel=w_p, strides=1, padding='valid') + self.epsilon\n",
        "        zn\n",
        "\n",
        "        s_p = r / z\n",
        "\n",
        "        c_p = K.tf.contrib.nn.conv1d_transpose(\n",
        "                value= s_p,\n",
        "                filter= w_p, \n",
        "                output_shape= K.shape(a),\n",
        "                stride= 1,\n",
        "                padding='SAME',\n",
        "                name=None\n",
        "                )\n",
        "\n",
        "        return(a*c_p)\n",
        "    '''\n",
        "\n",
        "    def backprop_conv(self, w, b, a, r):\n",
        "\n",
        "        w_p = K.maximum(w, 0.)\n",
        "        b_p = K.maximum(b, 0.)\n",
        "        z_p = K.conv1d(a, kernel=w_p, strides=1, padding='valid') + b_p + self.epsilon  \n",
        "        s_p = r / z_p\n",
        "        c_p = tf.contrib.nn.conv1d_transpose(\n",
        "                value= s_p,\n",
        "                filter= w_p, \n",
        "                output_shape= K.shape(a),\n",
        "                stride= 1,\n",
        "                padding='SAME',\n",
        "                name=None\n",
        "                )\n",
        "        w_n = K.minimum(w, 0.)\n",
        "        b_n = K.minimum(b, 0.)\n",
        "        z_n = K.conv1d(a, kernel=w_n, strides=1, padding='valid') + b_n - self.epsilon\n",
        "        s_n = r / z_n\n",
        "        c_n = tf.contrib.nn.conv1d_transpose(\n",
        "                value= s_n,\n",
        "                filter= w_n,\n",
        "                output_shape= K.shape(a),\n",
        "                stride= 1,\n",
        "                padding='SAME',\n",
        "                name=None\n",
        "                )\n",
        "\n",
        "        return a * (self.alpha * c_p + self.beta * c_n)\n",
        "\n",
        "\n",
        "\n",
        "    def predict_labels(self, samples):\n",
        "        return utils.predict_labels(self.model, samples)\n",
        "\n",
        "    def run_lrp(self, samples):\n",
        "        lrp_runner = K.function(inputs=[self.model.input, ], outputs=[self.relevance, ])        \n",
        "        return self.lrp_runner([samples, ])[0]\n",
        "\n",
        "    def compute_heatmap(self, samples, **kwargs):\n",
        "        lrp=self.run_lrp(samples)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    Test_X = np.array(pickle.load(open(\"Test_X.pickle\",\"rb\")))\n",
        "    Test_y = np.array(pickle.load(open(\"Test_y.pickle\",\"rb\")))\n",
        "    Test_X = Test_X/100.0\n",
        "    Test_X=np.reshape(Test_X, (Test_X.shape[0], Test_X.shape[1],1))\n",
        "\n",
        "    lrp=LayerwiseRelevancePropagation()\n",
        "    #lrp.print_model()\n",
        "\n",
        "    model=lrp.model\n",
        "    #sample= np.expand_dims(Test_X[1000], axis=0)\n",
        "    #print(model.predict_classes(sample))\n",
        "\n",
        "    ################## test backend function########################################\n",
        "    #a = tf.keras.layers.Flatten(input_shape=(28, 28))\n",
        "    #print(a)\n",
        "    #print([model.layers[0].input], [model.layers[3].output])\n",
        "    #print(len(Test_X), len(Test_X[0]))\n",
        "    get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[3].output])\n",
        "    layer_output = get_3rd_layer_output([Test_X])[0]\n",
        "    print(layer_output)\n",
        "\n",
        "    ################## test backend function########################################"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         ... 0.5314047  0.         0.        ]\n",
            " [0.         0.         0.         ... 0.83946216 0.         0.        ]\n",
            " [0.         0.         0.         ... 1.0210003  0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.2603058  0.         0.        ]\n",
            " [0.         0.         0.         ... 0.6221078  0.         0.        ]\n",
            " [0.         0.         0.         ... 0.56663203 0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}